{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"v2_RNN-LSTM-GRU.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"KziRLLKJNi57","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"uyrc4KKRNi6F","colab_type":"code","colab":{}},"source":["import re\n","import tensorflow as tf\n","import numpy as np\n","import string\n","\n","from tensorflow.keras import datasets, layers, models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"V0KiQpJNNi6M","colab_type":"code","colab":{}},"source":["def plot_accuracy (history):\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  plt.title('model accuracy')\n","  plt.xlabel('epoch')\n","  plt.ylabel('accuracy')\n","  plt.legend(['test', 'train'], loc='upper left')\n","  plt.show()\n","\n","def plot_loss (history):\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  plt.title('model loss')\n","  plt.xlabel('epoch')\n","  plt.ylabel('loss')\n","  plt.legend(['test', 'train'], loc='upper left')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"tZwMHeKpNi6T","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"zd1hA7TdNi6e","colab_type":"code","colab":{}},"source":["train_tuple, test_tuple = [], []\n","leave = set(string.punctuation)\n","\n","f_train = open(\"./../input/train.txt\")\n","f_test = open(\"./../input/test.txt\")\n","\n","idx = 0\n","while (idx < 25000):\n","    s = f_train.readline()\n","    s = s.replace(\"<br />\", ' ')\n","    s = (''.join(ch for ch in s if ch not in leave)).lower().split()\n","    if (len(s) == 0):\n","      print('shit')\n","    if (idx < 12500):\n","      train_tuple.append((s, 0))\n","    else:\n","      train_tuple.append((s, 1))\n","    idx += 1\n","\n","idx = 0\n","while (idx < 25000):\n","    s = f_test.readline()\n","    s = s.replace(\"<br />\", ' ')\n","    s = (''.join(ch for ch in s if ch not in leave)).lower().split()\n","    if (idx < 12500):\n","      test_tuple.append((s, 0))\n","    else:\n","      test_tuple.append((s, 1))\n","    idx += 1\n","\n","f_train.close()\n","f_test.close()\n","print(train_tuple[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"bnG7dPh8Ni6j","colab_type":"code","colab":{}},"source":["np.random.shuffle(train_tuple)\n","np.random.shuffle(test_tuple)\n","\n","validation_tuple = train_tuple [ : 5000]\n","train_tuple = train_tuple [5000 : ]\n","\n","print(np.shape(train_tuple))\n","print(np.shape(validation_tuple))\n","print(np.shape(test_tuple))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"RvwTMtWJNi6n","colab_type":"code","colab":{}},"source":["words = [train_tuple[i][0][j] for i in range(20000) for j in range(len(train_tuple[i][0]))]\n","vocab = sorted(list(set(words)))\n","vocab = set(vocab)\n","\n","word_to_int = dict((word, idx) for idx, word in enumerate(vocab))\n","int_to_word = dict((idx, word) for idx, word in enumerate(vocab))\n","\n","def encoding(ls):                             # encodes one review\n","  curr_review = ls\n","  curr_review_from_vocab = []\n","  # print(\"entry\")\n","  for word in curr_review:\n","    if word in vocab:\n","      curr_review_from_vocab.append(word_to_int[word])\n","  # print(\"exit\")\n","  ln = len(curr_review_from_vocab)\n","  for idx in range(200-ln):                   # padding with extra zeroes to for length 200\n","    curr_review_from_vocab.append(0.0)\n","  curr_review_from_vocab = curr_review_from_vocab[:200]\n","  review_np_array = np.array(curr_review_from_vocab, dtype=np.float32)\n","  return review_np_array\n","\n","\n","def get_encoding(list_of_reviews):\n","  seq, label = [], []\n","  for i in range(len(list_of_reviews)):\n","    review_np_array = encoding(list_of_reviews[i][0])\n","    seq.append(review_np_array)\n","    label.append(list_of_reviews[i][1])\n","  seq = np.asarray(seq)\n","  label = np.asarray(label)\n","  return seq, label\n","\n","\n","train_data, train_label = get_encoding(train_tuple)\n","validation_data, validation_label = get_encoding(validation_tuple)\n","test_data, test_label = get_encoding(test_tuple)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"Qr5qzXnYNi7f","colab_type":"code","colab":{}},"source":["\n","train_words = [train_tuple[i][0][j] for i in range(20000) for j in range(len(train_tuple[i][0]))]\n","test_words = [test_tuple[i][0][j] for i in range(25000) for j in range(len(test_tuple[i][0]))]\n","valid_words = [validation_tuple[i][0][j] for i in range(5000) for j in range(len(validation_tuple[i][0]))]\n","\n","vocab_total = sorted(list(set(train_words + test_words + valid_words)))\n","vocab_total = set(vocab_total)\n","\n","print(len(vocab_total))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"8s9Hesx4Ni7i","colab_type":"code","colab":{}},"source":["import gensim\n","from gensim import models\n","\n","from gensim.models import Word2Vec\n","from gensim import corpora\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, RegexpTokenizer\n","\n","english_stopwords = set(stopwords.words('english'))\n","\n","def clean(data):\n","    train = []\n","    for v in data:                                       # one review (v)\n","        temp = []\n","        temp = [w for w in v[0] if not w in english_stopwords]\n","        # temp = np.asarray(temp)\n","        train.append(temp)                                 # train is a list of sentence (list of list of words)\n","    # train = np.asarray(train)\n","    return train\n","\n","\n","# print(len(train_data_1))\n","# print(train_data_1[0])\n","\n","w2v_cbow = Word2Vec(size=128, window=5, min_count=5, alpha=0.01, min_alpha=0.001, workers=4)\n","w2v_sg = Word2Vec(size=128, window=5, min_count=5, alpha=0.01, min_alpha=0.001, workers=4, sg=1)\n","\n","w2v_cbow_1 = Word2Vec(size=128, window=5, min_count=5, alpha=0.01, min_alpha=0.001, workers=4)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ovU_NIznNi7m","colab_type":"code","colab":{}},"source":["w2v_cbow.build_vocab(train_data_1 + test_data_1 + validation_data_1, progress_per=1000)\n","w2v_sg.build_vocab(train_data_1 + test_data_1 + validation_data_1, progress_per=1000)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"HAkyi3X7Ni7q","colab_type":"code","colab":{}},"source":["w2v_cbow_1.build_vocab(train_data, progress_per=1000)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ybDvIpj_Ni7t","colab_type":"code","colab":{}},"source":["print (len(w2v_cbow.wv.vocab))\n","print (len(w2v_sg.wv.vocab))\n","\n","NUM_WORDS = len(w2v_cbow.wv.vocab)\n","print (w2v_cbow)\n","print (w2v_sg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"KHA3jwqYNi7y","colab_type":"code","colab":{}},"source":["# Train both models using the preprocessed training data\n","\n","w2v_cbow.train(train_data_1 + test_data_1 + validation_data_1, total_examples=w2v_cbow.corpus_count, epochs=30, report_delay=1)\n","# w2v_sg.train(train_data_1 + test_data_1 + validation_data_1, total_examples=w2v_sg.corpus_count, epochs=30, report_delay=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"cURBPz6aNi70","colab_type":"code","colab":{}},"source":["# Train both models using the preprocessed training data\n","\n","w2v_cbow_1.train(train_data_1, total_examples=w2v_cbow_1.corpus_count, epochs=30, report_delay=1)\n","# w2v_sg.train(train_data_1 + test_data_1 + validation_data_1, total_examples=w2v_sg.corpus_count, epochs=30, report_delay=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"JHGic3mdNi74","colab_type":"code","colab":{}},"source":["def review_embedding(model, review):\n","    vec = []\n","    for word in review:\n","        vec.append(model.wv[word])\n","    return vec\n","\n","def embed_dataset(model, list_of_tuples):\n","    dataset_vec = []\n","    for review in list_of_tuples:\n","        dataset_vec.append(review_embedding(model, review))\n","    return dataset_vec\n","\n","embedding_matrix = []\n","for word in w2v_cbow_1.wv.vocab:\n","    embedding_matrix.append(w2v_cbow_1.wv[word])\n","\n","embedding_matrix = np.asarray(embedding_matrix)\n","print(np.shape(embedding_matrix))\n","\n","def preprocess(list_tuple):\n","    dataset_vec = []\n","    for tup in list_tuple:\n","        one_review = []\n","        for word in tup[0]:\n","            if word in w2v_cbow_1.wv.vocab:\n","                one_review.append(w2v_cbow_1.wv[word])\n","        temp = []\n","        for i in range(200 - len(one_review)):\n","            temp.append([0 for _ in range(200)])\n","        dataset_vec.append(one_review + temp)\n","    return dataset_vec\n","\n","train_data_1, test_data_1, validation_data_1 = [], [], []\n","train_data_1 = preprocess(train_tuple)\n","test_data_1 = preprocess(test_tuple)\n","validation_data_1 = preprocess(validation_tuple)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"51-5lsVXNi76","colab_type":"code","colab":{}},"source":["'''\n","embedding_layer = Embedding(NUM_WORDS, 128, \n","                            weights=[embedding_matrix], \n","                            input_length=200, \n","                            trainable=False)\n","model_6.add(layers.Embedding(len(w2v_cbow_1.wv.vocab), \n","                             128, input_length=200, \n","                             weights=[embedding_matrix], \n","                             trainable=False))\n","\n","'''        \n","# Till now best model was Model 5\n","\n","import tensorflow\n","\n","model_6 = tensorflow.keras.models.Sequential()\n","model_6.add(layers.Dropout(0.25))\n","model_6.add(layers.GRU(200, activation='tanh', return_sequences=True))\n","model_6.add(layers.GRU(200, activation='tanh', return_sequences=True))\n","model_6.add(layers.GRU(200, activation='tanh', return_sequences=False))\n","\n","model_6.add(layers.Dense(1))\n","model_6.add(layers.Activation('sigmoid'))\n","model_6.build((None, 200, 128))\n","\n","model_6.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"tRRvS2naNi79","colab_type":"code","colab":{}},"source":["model_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history_6 = model_6.fit(train_data, train_label, epochs=2, validation_data=(validation_data, validation_label))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"6pMR5XIzNi8B","colab_type":"code","colab":{}},"source":["score_6 = model_6.evaluate(test_data, test_label, verbose=0)\n","print(score_6)\n","score_6_ = model_6.evaluate(validation_data, validation_label, verbose=0)\n","print(score_6_)\n","score_6__ = model_6.evaluate(train_data, train_label, verbose=0)\n","print(score_6__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QL5JiD25N7TK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":133},"outputId":"0b224cbe-75fc-4cbc-c8cf-3791c0a9377f","executionInfo":{"status":"error","timestamp":1572834551868,"user_tz":-330,"elapsed":1014,"user":{"displayName":"Shruti Umat","photoUrl":"","userId":"14687378247255361715"}}},"source":["import pandas as pd\n","import numpy as np\n","\n","columns = []\n","columns.append('Training Accuracy (%)')\n","columns.append('Validation Accuracy (%)')\n","columns.append('Test Accuracy (%)')\n","\n","\n","df = pd.DataFrame({'Model': np.linspace(1, 9, 9)})\n","df = pd.concat([df, pd.DataFrame(np.random.randn(9, 5), columns=columns)],\n","               axis=1)\n","\n","df.iloc[0, 1] = \n","df.iloc[0, 2] = \n","df.iloc[0, 3] = \n","\n","df.iloc[1, 1] = 63.46\n","df.iloc[1, 2] = \n","df.iloc[1, 3] = \n","\n","df.iloc[2, 1] = \n","df.iloc[2, 2] = \n","df.iloc[2, 3] = \n","\n","df.iloc[3, 1] = 99.89\n","df.iloc[3, 2] = 87.64\n","df.iloc[3, 3] = 84.02\n","\n","df.iloc[4, 1] = 99.71\n","df.iloc[4, 2] = 86.86\n","df.iloc[4, 3] = 83.58\n","\n","df"],"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c3a5a74132c4>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    df.iloc[0, 1] =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}]}